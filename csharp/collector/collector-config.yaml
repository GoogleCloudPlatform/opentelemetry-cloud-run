receivers:
  otlp:
    protocols:
      grpc:
      http:

processors:
  batch:
    # batch metrics before sending to reduce API usage
    send_batch_max_size: 200
    send_batch_size: 200
    timeout: 5s

  memory_limiter:
    # drop metrics if memory usage gets too high
    check_interval: 1s
    limit_percentage: 65
    spike_limit_percentage: 20

  # automatically detect Cloud Run resource metadata
  resourcedetection:
    detectors: [env, gcp]
    timeout: 2s
    override: false

  resource:
    attributes:
    # add instance_id as a resource attribute
    - key: service.instance.id
      from_attribute: faas.id
      action: upsert
      # parse service name from K_SERVICE Cloud Run variable
    - key: service.name
      value: ${env:K_SERVICE}
      action: insert

  # Required for sending telemetry to telemetry.googleapis.com
  resource/gcp_project_id:
    attributes:
    - key: gcp.project_id
      value: "%GOOGLE_CLOUD_PROJECT%"
      action: insert

  metricstarttime:
    strategy: subtract_initial_point

exporters:
  googlemanagedprometheus: # Note: this is intentionally left blank
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200
  googlecloud:
    log:
      default_log_name: opentelemetry-collector
  otlphttp:
    encoding: proto
    endpoint: https://telemetry.googleapis.com
    auth:
      authenticator: googleclientauth

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  googleclientauth:

service:
  extensions:
  - health_check
  - googleclientauth
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, metricstarttime, resourcedetection, resource, batch]
      exporters: [googlemanagedprometheus, debug]
    traces:
      receivers: [otlp]
      processors: [batch, memory_limiter, resource/gcp_project_id, resourcedetection]
      exporters: [otlphttp, debug]
    logs:
      receivers: [otlp]
      processors: [resourcedetection, memory_limiter, batch]
      exporters: [googlecloud]
